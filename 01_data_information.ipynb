{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle Hub setup\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=\"C:/Users/russe/OneDrive/Desktop/Portfolio/Next AAA Title\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imdb_data(data_path):\n",
    "    \"\"\"\n",
    "    Load IMDb data with memory optimization and business focus\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üìä Loading IMDb data with memory optimization...\")\n",
    "    \n",
    "    # Load movies with minimal columns for initial filtering\n",
    "    print(\"  üé¨ Loading movies (basic info only)...\")\n",
    "    movies_basic = pd.read_csv(\n",
    "        f'{data_path}/Data/title.basics.tsv',\n",
    "        sep='\\t',\n",
    "        na_values='\\\\N',\n",
    "        usecols=['tconst', 'titleType', 'primaryTitle', 'startYear', 'genres', 'runtimeMinutes']\n",
    "    )\n",
    "    \n",
    "    # Filter for movies 2010+ only (business relevance + memory efficiency)\n",
    "    movies = movies_basic[\n",
    "        (movies_basic['titleType'] == 'movie') & \n",
    "        (movies_basic['startYear'] >= 2010)\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"    ‚úÖ Filtered to {len(movies):,} recent movies (2010+)\")\n",
    "    \n",
    "    # Load ratings\n",
    "    print(\"  ‚≠ê Loading ratings...\")\n",
    "    ratings = pd.read_csv(\n",
    "        f'{data_path}/Data/title.ratings.tsv',\n",
    "        sep='\\t'\n",
    "    )\n",
    "    \n",
    "    # Merge and filter for movies with decent vote counts\n",
    "    movies_with_ratings = movies.merge(ratings, on='tconst', how='inner')\n",
    "    \n",
    "    # Filter for movies with at least 50 votes (quality threshold)\n",
    "    movies_with_ratings = movies_with_ratings[movies_with_ratings['numVotes'] >= 50].copy()\n",
    "    \n",
    "    print(f\"    ‚úÖ Final dataset: {len(movies_with_ratings):,} movies with quality ratings\")\n",
    "    \n",
    "    return movies, ratings, movies_with_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_actors(data_path, movies_df):\n",
    "    \"\"\"\n",
    "    Load actor data efficiently for 2020 analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"  üë• Loading actor data (targeted)...\")\n",
    "    \n",
    "    # Get list of movie IDs for 2020 to reduce data loading\n",
    "    target_years = [2017, 2018, 2019, 2020, 2021]  # Expand slightly for better sample\n",
    "    target_movies = movies_df[movies_df['startYear'] >= 2010]['tconst'].unique()\n",
    "    \n",
    "    print(f\"    Target movies for actor analysis: {len(target_movies):,}\")\n",
    "    \n",
    "    # Load principals with filtering\n",
    "    chunk_size = 1000000\n",
    "    principals_filtered = []\n",
    "    \n",
    "    print(\"    Loading principals in chunks...\")\n",
    "    for chunk in pd.read_csv(\n",
    "        f'{data_path}/Data/title.principals.tsv',\n",
    "        sep='\\t',\n",
    "        na_values='\\\\N',\n",
    "        chunksize=chunk_size,\n",
    "        usecols=['tconst', 'nconst', 'category', 'ordering']\n",
    "    ):\n",
    "        # Filter for target movies and actors only\n",
    "        filtered_chunk = chunk[\n",
    "            (chunk['tconst'].isin(target_movies)) &\n",
    "            (chunk['category'].isin(['actor', 'actress']))\n",
    "        ]\n",
    "        \n",
    "        if len(filtered_chunk) > 0:\n",
    "            principals_filtered.append(filtered_chunk)\n",
    "    \n",
    "    if principals_filtered:\n",
    "        principals = pd.concat(principals_filtered, ignore_index=True)\n",
    "        print(f\"    ‚úÖ Actor connections loaded: {len(principals):,}\")\n",
    "    else:\n",
    "        print(\"    ‚ö†Ô∏è No actor data found for target years\")\n",
    "        principals = pd.DataFrame()\n",
    "    \n",
    "    # Load names for actors in our dataset\n",
    "    if len(principals) > 0:\n",
    "        target_actors = principals['nconst'].unique()\n",
    "        \n",
    "        print(f\"    Loading names for {len(target_actors):,} actors...\")\n",
    "        \n",
    "        # Read names in chunks and filter\n",
    "        names_filtered = []\n",
    "        for chunk in pd.read_csv(\n",
    "            f'{data_path}/Data/name.basics.tsv',\n",
    "            sep='\\t',\n",
    "            na_values='\\\\N',\n",
    "            chunksize=500000,\n",
    "            usecols=['nconst', 'primaryName']\n",
    "        ):\n",
    "            filtered_chunk = chunk[chunk['nconst'].isin(target_actors)]\n",
    "            if len(filtered_chunk) > 0:\n",
    "                names_filtered.append(filtered_chunk)\n",
    "        \n",
    "        if names_filtered:\n",
    "            names = pd.concat(names_filtered, ignore_index=True)\n",
    "            print(f\"    ‚úÖ Actor names loaded: {len(names):,}\")\n",
    "        else:\n",
    "            names = pd.DataFrame()\n",
    "    else:\n",
    "        names = pd.DataFrame()\n",
    "    \n",
    "    return principals, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loading IMDb data with memory optimization...\n",
      "  üé¨ Loading movies (basic info only)...\n",
      "    ‚úÖ Filtered to 282,615 recent movies (2010+)\n",
      "  ‚≠ê Loading ratings...\n",
      "    ‚úÖ Final dataset: 83,583 movies with quality ratings\n",
      "  üë• Loading actor data (targeted)...\n",
      "    Target movies for actor analysis: 282,615\n",
      "    Loading principals in chunks...\n",
      "    ‚úÖ Actor connections loaded: 1,516,184\n",
      "    Loading names for 749,122 actors...\n",
      "    ‚úÖ Actor names loaded: 748,836\n"
     ]
    }
   ],
   "source": [
    "movies_df, ratings_df, movies_ratings_df = load_imdb_data(data_path)\n",
    "principals_df, names_df = load_actors(data_path, movies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.to_csv(\"movies.csv\")\n",
    "ratings_df.to_csv(\"ratings.csv\")\n",
    "movies_ratings_df.to_csv(\"movies_ratings.csv\")\n",
    "principals_df.to_csv(\"principals.csv\")\n",
    "names_df.to_csv(\"names.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_information(movies, ratings, names, principals):\n",
    "    \"\"\"\n",
    "    Comprehensive data quality assessment for business decision-making\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"üîç COMPREHENSIVE DATA QUALITY ASSESSMENT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create primary analysis dataset\n",
    "    movies_with_ratings = movies.merge(ratings, on='tconst', how='inner')\n",
    "    print(f\"\\nüìä Primary Dataset Created:\")\n",
    "    print(f\"  Movies with ratings: {len(movies_with_ratings):,}\")\n",
    "    print(f\"  Coverage: {len(movies_with_ratings)/len(movies)*100:.1f}% of movies have ratings\")\n",
    "    \n",
    "    # Temporal coverage analysis\n",
    "    print(f\"\\nüìÖ Temporal Coverage Analysis:\")\n",
    "    year_stats = movies['startYear'].describe()\n",
    "    print(f\"  Year range: {year_stats['min']:.0f} - {year_stats['max']:.0f}\")\n",
    "    print(f\"  Movies per decade:\")\n",
    "    \n",
    "    # Movies by decade\n",
    "    decade_counts = movies[movies['startYear'] >= 1980].copy()\n",
    "    decade_counts['decade'] = (decade_counts['startYear'] // 10) * 10\n",
    "    decade_summary = decade_counts.groupby('decade').size()\n",
    "    \n",
    "    for decade, count in decade_summary.tail(5).items():\n",
    "        print(f\"    {int(decade)}s: {count:,} movies\")\n",
    "    \n",
    "    # Focus on recent movies (business relevance)\n",
    "    recent_movies = movies_with_ratings[movies_with_ratings['startYear'] >= 2015]\n",
    "    print(f\"\\nüéØ Business-Relevant Dataset (2015+):\")\n",
    "    print(f\"  Recent movies: {len(recent_movies):,} ({len(recent_movies)/len(movies_with_ratings)*100:.1f}%)\")\n",
    "    \n",
    "    # Rating quality analysis\n",
    "    print(f\"\\n‚≠ê Rating Quality Metrics:\")\n",
    "    print(f\"  Average rating: {movies_with_ratings['averageRating'].mean():.2f}/10\")\n",
    "    print(f\"  Rating standard deviation: {movies_with_ratings['averageRating'].std():.2f}\")\n",
    "    print(f\"  Median votes: {movies_with_ratings['numVotes'].median():,.0f}\")\n",
    "    \n",
    "    # Vote distribution analysis\n",
    "    vote_thresholds = [100, 1000, 10000, 100000]\n",
    "    print(f\"\\n  Vote credibility analysis:\")\n",
    "    for threshold in vote_thresholds:\n",
    "        count = len(movies_with_ratings[movies_with_ratings['numVotes'] >= threshold])\n",
    "        pct = count / len(movies_with_ratings) * 100\n",
    "        print(f\"    {threshold:,}+ votes: {count:,} movies ({pct:.1f}%)\")\n",
    "    \n",
    "    \n",
    "    # Genre analysis\n",
    "    print(f\"\\nüé≠ Genre Coverage Analysis:\")\n",
    "    all_genres = []\n",
    "    for genres in movies['genres'].dropna():\n",
    "        all_genres.extend([g.strip() for g in genres.split(',')])\n",
    "    \n",
    "    genre_counts = pd.Series(all_genres).value_counts()\n",
    "    print(f\"  Total unique genres: {len(genre_counts)}\")\n",
    "    print(f\"  Top 5 genres:\")\n",
    "    for genre, count in genre_counts.head(5).items():\n",
    "        print(f\"    {genre}: {count:,} movies\")\n",
    "    \n",
    "    \n",
    "    return movies_with_ratings, recent_movies, genre_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üîç COMPREHENSIVE DATA QUALITY ASSESSMENT\n",
      "============================================================\n",
      "\n",
      "üìä Primary Dataset Created:\n",
      "  Movies with ratings: 152,447\n",
      "  Coverage: 53.9% of movies have ratings\n",
      "\n",
      "üìÖ Temporal Coverage Analysis:\n",
      "  Year range: 2010 - 2032\n",
      "  Movies per decade:\n",
      "    2010s: 168,786 movies\n",
      "    2020s: 113,823 movies\n",
      "    2030s: 6 movies\n",
      "\n",
      "üéØ Business-Relevant Dataset (2015+):\n",
      "  Recent movies: 110,282 (72.3%)\n",
      "\n",
      "‚≠ê Rating Quality Metrics:\n",
      "  Average rating: 6.27/10\n",
      "  Rating standard deviation: 1.52\n",
      "  Median votes: 64\n",
      "\n",
      "  Vote credibility analysis:\n",
      "    100+ votes: 65,190 movies (42.8%)\n",
      "    1,000+ votes: 22,331 movies (14.6%)\n",
      "    10,000+ votes: 5,539 movies (3.6%)\n",
      "    100,000+ votes: 1,186 movies (0.8%)\n",
      "\n",
      "üé≠ Genre Coverage Analysis:\n",
      "  Total unique genres: 26\n",
      "  Top 5 genres:\n",
      "    Drama: 96,581 movies\n",
      "    Documentary: 88,425 movies\n",
      "    Comedy: 45,882 movies\n",
      "    Thriller: 22,440 movies\n",
      "    Horror: 21,326 movies\n"
     ]
    }
   ],
   "source": [
    "movies_with_ratings, recent_movies, genre_counts = data_information(\n",
    "    movies_df, ratings_df, names_df, principals_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_business_ready_dataset(movies_with_ratings):\n",
    "    \"\"\"\n",
    "    Create analysis-ready dataset with business-relevant features\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nüéØ Creating Business-Ready Dataset...\")\n",
    "    \n",
    "    # Focus on movies with sufficient data for analysis\n",
    "    business_dataset = movies_with_ratings.copy()\n",
    "    \n",
    "    # Add derived business metrics\n",
    "    print(\"  üìä Adding business metrics...\")\n",
    "    \n",
    "    # Weighted rating (IMDb formula)\n",
    "    m = 1000  # minimum votes threshold\n",
    "    C = business_dataset['averageRating'].mean()\n",
    "    \n",
    "    business_dataset['weighted_rating'] = (\n",
    "        (business_dataset['numVotes'] / (business_dataset['numVotes'] + m)) * business_dataset['averageRating'] +\n",
    "        (m / (business_dataset['numVotes'] + m)) * C\n",
    "    )\n",
    "    \n",
    "    # Credibility flag\n",
    "    business_dataset['high_credibility'] = business_dataset['numVotes'] >= 1000\n",
    "    \n",
    "    # Recent movie flag\n",
    "    business_dataset['recent_movie'] = business_dataset['startYear'] >= 2015\n",
    "    \n",
    "    # Genre processing\n",
    "    business_dataset['genre_count'] = business_dataset['genres'].str.count(',') + 1\n",
    "    business_dataset['genre_count'] = business_dataset['genre_count'].fillna(0)\n",
    "\n",
    "    business_dataset['runtimeMinutes'] = pd.to_numeric(business_dataset['runtimeMinutes'], errors='coerce')\n",
    "    business_dataset['runtimeMinutes_filled'] = business_dataset['runtimeMinutes'].fillna(120)  # Default 2 hours\n",
    "    \n",
    "    # Runtime categories\n",
    "    business_dataset['runtime_category'] = pd.cut(\n",
    "        business_dataset['runtimeMinutes'],\n",
    "        bins=[0, 90, 120, 150, 300],\n",
    "        labels=['Short', 'Standard', 'Long', 'Very Long'],\n",
    "        include_lowest=True\n",
    "    )\n",
    "    \n",
    "    print(f\"  ‚úÖ Business dataset ready: {len(business_dataset):,} movies\")\n",
    "    print(f\"    High credibility movies: {business_dataset['high_credibility'].sum():,}\")\n",
    "    print(f\"    Recent movies (2015+): {business_dataset['recent_movie'].sum():,}\")\n",
    "    \n",
    "    return business_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Creating Business-Ready Dataset...\n",
      "  üìä Adding business metrics...\n",
      "  ‚úÖ Business dataset ready: 152,447 movies\n",
      "    High credibility movies: 22,331\n",
      "    Recent movies (2015+): 110,282\n"
     ]
    }
   ],
   "source": [
    "business_ready_data = create_business_ready_dataset(movies_with_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_summary(business_dataset, genre_counts):\n",
    "    \"\"\"\n",
    "    Generate executive summary of data assets\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"üìã EXECUTIVE DATA SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Dataset overview\n",
    "    print(f\"üé¨ MOVIE DATABASE OVERVIEW:\")\n",
    "    print(f\"  Total movies: {len(business_dataset):,}\")\n",
    "    print(f\"  Time period: {business_dataset['startYear'].min():.0f} - {business_dataset['startYear'].max():.0f}\")\n",
    "    print(f\"  Average rating: {business_dataset['averageRating'].mean():.2f}/10\")\n",
    "    print(f\"  Total user votes: {business_dataset['numVotes'].sum():,}\")\n",
    "    \n",
    "    # Business-relevant segments\n",
    "    print(f\"\\nüìä BUSINESS-RELEVANT SEGMENTS:\")\n",
    "    recent_high_cred = business_dataset[\n",
    "        business_dataset['recent_movie'] & business_dataset['high_credibility']\n",
    "    ]\n",
    "    print(f\"  Recent + High Credibility: {len(recent_high_cred):,} movies\")\n",
    "    print(f\"  AAA Analysis Ready: {len(recent_high_cred):,} movies\")\n",
    "    \n",
    "    # 2020 specific (for business questions)\n",
    "    movies_2020 = business_dataset[business_dataset['startYear'] == 2020]\n",
    "    print(f\"  2020 movies: {len(movies_2020):,}\")\n",
    "    print(f\"  2020 with high credibility: {len(movies_2020[movies_2020['high_credibility']]):,}\")\n",
    "    \n",
    "    # Genre diversity\n",
    "    print(f\"\\nüé≠ CONTENT DIVERSITY:\")\n",
    "    print(f\"  Unique genres: {len(genre_counts)}\")\n",
    "    print(f\"  Most common: {genre_counts.head(3).index.tolist()}\")\n",
    "    \n",
    "    # Data quality score\n",
    "    completeness_score = (\n",
    "        (business_dataset['averageRating'].notna().sum() / len(business_dataset)) * 0.3 +\n",
    "        (business_dataset['numVotes'].notna().sum() / len(business_dataset)) * 0.3 +\n",
    "        (business_dataset['genres'].notna().sum() / len(business_dataset)) * 0.2 +\n",
    "        (business_dataset['runtimeMinutes'].notna().sum() / len(business_dataset)) * 0.2\n",
    "    ) * 100\n",
    "    \n",
    "    print(f\"\\n‚úÖ DATA QUALITY SCORE: {completeness_score:.1f}/100\")\n",
    "    print(f\"   Ready for advanced analytics: {'YES' if completeness_score > 80 else 'NEEDS IMPROVEMENT'}\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(f\"\\nüí° RECOMMENDATIONS:\")\n",
    "    if len(recent_high_cred) >= 1000:\n",
    "        print(\"  ‚úÖ Sufficient data for robust AAA analysis\")\n",
    "    else:\n",
    "        print(\"  ‚ö†Ô∏è Consider expanding time window for larger sample size\")\n",
    "    \n",
    "    if completeness_score > 85:\n",
    "        print(\"  ‚úÖ High data quality - proceed with confidence\")\n",
    "    else:\n",
    "        print(\"  ‚ö†Ô∏è Address missing data before critical analysis\")\n",
    "    \n",
    "    print(\"  üìà Ready for EDA and business question analysis\")\n",
    "    \n",
    "    return {\n",
    "        'total_movies': len(business_dataset),\n",
    "        'recent_high_cred': len(recent_high_cred),\n",
    "        'movies_2020': len(movies_2020),\n",
    "        'data_quality_score': completeness_score,\n",
    "        'genre_diversity': len(genre_counts)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_summary = generate_data_summary(business_ready_data, genre_counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
